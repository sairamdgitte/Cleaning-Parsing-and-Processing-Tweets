{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FIT5196 Task 2 in Assessment 1\n",
    "#### Student Name: Sai Ram D Gitte\n",
    "#### Student ID: 31009751\n",
    "\n",
    "Date: 13/09/2020\n",
    "\n",
    "Version: 1.0\n",
    "\n",
    "Environment: Python 3.7.4 and Jupyter notebook\n",
    "\n",
    "Libraries used: please include the main libraries you used in your assignment here, e.g.,:\n",
    "* pandas (for dataframe) \n",
    "* re (for regular expression) \n",
    "* langid (for language classification) \n",
    "* numpy (for numpy array) \n",
    "* nltk (for natural language processing)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Libraries \n",
    "import pandas as pd \n",
    "import langid \n",
    "import numpy as np\n",
    "import re\n",
    "from nltk.probability import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading the file and its sheets\n",
    "- reading the xlsx file in pandas \n",
    "- cleaning the header, dropping all the Nan's and NAs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sheet_list_df = pd.read_excel(\"31009751.xlsx\", sheet_name = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterating through each dataframe in sheet_list_df and to get rid of all the 'nan' is all the columns and rows\n",
    "list_of_dfs = []\n",
    "\n",
    "for key, value in sheet_list_df.items():\n",
    "    \n",
    "        sheet_list_df_2 = sheet_list_df[key].dropna(axis=0, how=\"all\")\n",
    "        sheet_list_df_3 = sheet_list_df_2.dropna(axis=1, how=\"all\")\n",
    "\n",
    "\n",
    "        if ~np.isin(['text'], sheet_list_df_3.columns):\n",
    "            # Cleaning the header part of each dataframe \n",
    "            # store 'text','id', and 'created_at' row to make it the header\n",
    "            new_header = sheet_list_df_3.iloc[0]\n",
    "\n",
    "            # Select all the rows below the header to be the rows of the data frame\n",
    "            sheet_list_df_3 = sheet_list_df_3[1:]                             \n",
    "\n",
    "            # Place the stored header name as the header for the new dataframe\n",
    "            sheet_list_df_3.columns = new_header\n",
    "\n",
    "            # Store all the dataframes as an element in a list\n",
    "            list_of_dfs.append(sheet_list_df_3)\n",
    "            \n",
    "        else:\n",
    "            sheet_list_df_2 = sheet_list_df[key].dropna(axis=0, how=\"all\")\n",
    "            sheet_list_df_3 = sheet_list_df_2.dropna(axis=1, how=\"all\")\n",
    "            \n",
    "            list_of_dfs.append(sheet_list_df_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language classification and tokenization\n",
    "- determining the language of each text\n",
    "- retaining only the english tweets\n",
    "- tokenizing all the texts \n",
    "- takes about 20mins to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retaining only the english tweets classified by langid\n",
    "for i in range(len(list_of_dfs)):\n",
    "    list_of_dfs[i]['language'] = list_of_dfs[i]['text'].apply(lambda x: langid.classify(x)[0] if not str(x).isdigit() else None)\n",
    "    list_of_dfs[i] = list_of_dfs[i][list_of_dfs[i]['language'] == 'en']\n",
    "    list_of_dfs[i]['tokens'] = list_of_dfs[i]['text'].apply(lambda x: str(re.findall(\"[a-zA-Z]+(?:[-'][a-zA-Z]+)?\", x.lower())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting the overall words used in all of the sheets in just one list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "token_list = []\n",
    "\n",
    "for i in list_of_dfs:\n",
    "    sub_token_list = []\n",
    "    for j in range(len(i['tokens'])):\n",
    "        sub_token_list.append(eval(i['tokens'].values[j]))\n",
    "    token_list.append(list(itertools.chain.from_iterable(sub_token_list)))   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flattening the list of lists and having a copy stored \n",
    "all_words = list(itertools.chain.from_iterable(token_list))\n",
    "all_words_dup = list(itertools.chain.from_iterable(token_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Context independent removal \n",
    "- reading the stop words file\n",
    "- getting rid of all the independent stop words from the flattened list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing context independent words\n",
    "stop_words = pd.read_csv(\"stopwords_en.txt\", names=['words'])\n",
    "\n",
    "for word in stop_words['words'].values:\n",
    "    if word in all_words:\n",
    "        all_words_dup.pop(all_words_dup.index(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Putting pandas to good use!!\n",
    "### To eleminate context dependent words from the doc\n",
    "- reading each day's tokenized words into columns\n",
    "- having a separate dataframe to hold the words and their repsective counts\n",
    "- matching the words in the dataframe to each columns of the main dataframe and then counting the frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_df = pd.DataFrame(token_list).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words_df_count = pd.DataFrame(set(all_words_dup))\n",
    "all_words_df_count['count'] = [0 for i in range(all_words_df_count.shape[0])]\n",
    "all_words_df_count = all_words_df_count.set_index(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in actual_df.columns:\n",
    "    matching_words = np.intersect1d(all_words_df_count.index.values, np.unique(actual_df[i].values[actual_df[i].values != np.array(None)]), return_indices = True )[1]\n",
    "    all_words_df_count.iloc[matching_words,:] += 1\n",
    "all_words_df_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering out the context dependent words and the rare tokens simultaneously"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_dependent_words = all_words_df_count[all_words_df_count['count'] > 60]\n",
    "filtered_df = all_words_df_count[all_words_df_count['count'] <= 60] # Conext dependent words removal\n",
    "filtered_df = filtered_df[filtered_df['count'] >= 5] # Rare tokens removal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bigrams from the entire document\n",
    "- nltk package is used to determine the top 200 bigrams from the corpus\n",
    "- any bigram containing independent stop word is not included\n",
    "- extracting the bigrams and their frequncy counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "bigram_finder = nltk.collocations.BigramCollocationFinder.from_words(all_words_dup)\n",
    "bigram_finder.apply_freq_filter(70)\n",
    "bigram_finder.apply_word_filter(lambda w: len(w) < 3 or w.lower() in stop_words['words'].values)\n",
    "\n",
    "top_200_bigrams = bigram_finder.nbest(bigram_measures.pmi, 200) # Top-200 bigrams\n",
    "top_200_bigrams\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the count of each bigram from the top 200 bigrams we saw above\n",
    "bigrams_count = []\n",
    "for i in top_200_bigrams:\n",
    "    for key,value in dict(bigram_finder.ngram_fd.items()).items():  \n",
    "        if i == key:\n",
    "            bigrams_count.append(key[0]+\"_\"+key[1]+\":\"+str(value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using dataframe to hold the word count \n",
    "- each word is taken in a row\n",
    "- each word is stemmed using porter stemmer\n",
    "- list of string of each word and its freq is stored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "port_stem = PorterStemmer()\n",
    "\n",
    "\n",
    "# dup_df = pd.DataFrame(columns= ['word'])\n",
    "filtered_df['word'] = filtered_df.index.values\n",
    "filtered_df['stemmed'] = filtered_df['word'].apply(lambda x: port_stem.stem(x))\n",
    "single_words = filtered_df['stemmed'].values+ np.array([':']) + filtered_df['count'].values.astype(str).astype('object')\n",
    "single_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- the words and their counts are taken in two diff columns\n",
    "- the repeating words are groupedby and their counts are summed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = sorted(list(single_words) + bigrams_count)\n",
    "\n",
    "# To sum up the repeating word's count\n",
    "dffff = pd.DataFrame(vocab)\n",
    "dffff['count'] = dffff[0].apply(lambda x: int(x.split(':')[1]))\n",
    "dffff[0] = dffff[0].apply(lambda x: (x.split(':')[0]))\n",
    "dffff = dffff.groupby([0]).sum()\n",
    "dffff['length'] = dffff.reset_index()[0].apply(lambda x: len(x)).values\n",
    "dffff = dffff[dffff['length'] >= 3]\n",
    "total_words_count = dffff.index.values+ np.array([':']) + dffff['count'].values.astype(str).astype('object')\n",
    "total_words_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Writing the data into a txt file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"31009751_vocab.txt\", \"w\")\n",
    "for i in total_words_count:\n",
    "    f.write(i +'\\n')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sparse Matrix\n",
    "- for each of the tokenized words calculating its count and then repeating itself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sparse matrix calculation logic for the entire document\n",
    "vfu = np.vectorize(lambda t: str(t+' ') )\n",
    "stri = vfu(filtered_df.index.values) \n",
    "sparse_words = stri.astype('object') * filtered_df['count'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "X = vectorizer.fit_transform(sparse_words)\n",
    "\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2 .2\n",
    "- Bigrams and unigrams for each day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Context Independent Words Removal \n",
    "\n",
    "duplicate = [j for j in token_list]\n",
    "\n",
    "for stop_word in stop_words['words'].values:\n",
    "    for k in range(len(duplicate)):\n",
    "        if stop_word in token_list[k]:\n",
    "            duplicate[k].pop(duplicate[k].index(stop_word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Context dependent words removal \n",
    "list_df_words = []\n",
    "for i in range(len(duplicate)):\n",
    "    compare_df = pd.DataFrame(set(duplicate[i]))\n",
    "    compare_df['count'] = [0 for k in range(compare_df.shape[0])]\n",
    "    compare_df = compare_df.set_index(0)\n",
    "    \n",
    "    for j in range(len(duplicate)):\n",
    "        matching = np.intersect1d(compare_df.index.values, np.unique(np.array(duplicate[j])[np.array(duplicate[j]) != np.array(None)]), assume_unique = True, return_indices = True )[1]\n",
    "        compare_df.iloc[matching,:] += 1\n",
    "    list_df_words.append(compare_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rare tokens and context dependent words removal\n",
    "for i in range(len(list_df_words)):\n",
    "    list_df_words[i] = list_df_words[i][list_df_words[i]['count'] <= 60]\n",
    "    list_df_words[i] = list_df_words[i][list_df_words[i]['count'] >= 5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "bigram_per_day = []\n",
    "for i in range(len(token_list)):\n",
    "    bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "    bigram_finder = nltk.collocations.BigramCollocationFinder.from_words(token_list[i])\n",
    "    bigram_finder.apply_freq_filter(3)\n",
    "    bigram_finder.apply_word_filter(lambda w: len(w) < 3 or w.lower() in stop_words['words'].values)\n",
    "\n",
    "    top_100_bigrams = bigram_finder.nbest(bigram_measures.pmi, 100) # Top-100 bigrams\n",
    "    bigram_per_day.append(list(bigram_finder.ngram_fd.items()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the sheets names\n",
    "sheets = pd.ExcelFile(\"31009751.xlsx\")\n",
    "sheets = sheets.sheet_names\n",
    "bigram_count_dict = dict(zip(sheets, bigram_per_day))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Writing the data to a txt file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('31009751_100bi.txt', 'w')\n",
    "for key,value in bigram_count_dict.items():\n",
    "    f.write(key +\":\"+ str(value)+'\\n')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Top 100 unigram per day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.probability import FreqDist\n",
    "from nltk.util import ngrams\n",
    "\n",
    "\n",
    "unigram_list = []\n",
    "for i in range(len(token_list)):\n",
    "    bigrams = ngrams(token_list[i], 1)\n",
    "    \n",
    "    dict_of_bigrams = dict(FreqDist(bigrams))\n",
    "    \n",
    "    unigram_per_day = dict(sorted(dict_of_bigrams.items(), key=lambda x: x[1], reverse=True))\n",
    "    \n",
    "    unigram_without_stop = [(port_stem.stem(key[0]),value) for key,value in unigram_per_day.items() if len(key[0]) >= 3 if not any(stop in key for stop in stop_words['words'].values)]\n",
    "    unigram_list.append(unigram_without_stop[0:100])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram_count_daywise = dict(zip(sheets,unigram_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('31009751_100uni.txt', 'w')\n",
    "for key,value in unigram_count_daywise.items():\n",
    "    f.write(str(key) +\":\"+ str(value)+'\\n')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2 .3 Sparse Matrix per day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sparse matrix calculation logic starts from here\n",
    "vfu = np.vectorize(lambda t: str(t+' ') )\n",
    "stri = vfu(filtered_df.index.values) \n",
    "sparse_words = stri.astype('object') * filtered_df['count'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "X = vectorizer.fit_transform(sparse_words)\n",
    "\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for columns in range(len(list_df_words)):\n",
    "    \n",
    "    \n",
    "    list_df_words[columns]['stem'] = list_df_words[columns].index.values\n",
    "    list_df_words[columns]['stem'] = list_df_words[columns]['stem'].apply(lambda x: port_stem.stem(x))\n",
    "    list_df_words[columns]['length'] = list_df_words[columns]['stem'].apply(lambda x: len(x))\n",
    "    list_df_words[columns] = list_df_words[columns][list_df_words[columns]['length'] >= 3]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sparse matrix calculation logic starts from here\n",
    "total_sparse_words = []\n",
    "vfu = np.vectorize(lambda t: str(t+' ') )\n",
    "\n",
    "for i in list_df_words:\n",
    "    \n",
    "    stri = vfu(i['stem'].values) \n",
    "    sparse_words = stri.astype('object') * i['count'].values\n",
    "    total_sparse_words.append(sparse_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Writing the data to a txt file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "f = open('31009751_CountVect.txt', 'w')\n",
    "for k in total_sparse_words:\n",
    "    X = vectorizer.fit_transform(k)\n",
    "\n",
    "for m in sheets:\n",
    "    f.write(m + \",\")\n",
    "    for item in X.toarray():\n",
    "        if len(item[item.nonzero()]) == 1:\n",
    "            f.write(str(int(item.nonzero()[0])) + \":\" + str(int(item[item.nonzero()]))+\",\")\n",
    "            \n",
    "        else:\n",
    "            f.write(str(item.nonzero()[0][0]) + \":\" + str(item[item.nonzero()][0])+\",\")\n",
    "            f.write(str(item.nonzero()[0][1]) + \":\" + str(item[item.nonzero()][1])+\",\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
