{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FIT5196 Task 1 in Assessment 1\n",
    "#### Student Name: Sai Ram D Gitte\n",
    "#### Student ID: 31009751\n",
    "\n",
    "Date: 13/09/2020\n",
    "\n",
    "Version: 1.0\n",
    "\n",
    "Environment: Python 3.7.4 and Jupyter notebook\n",
    "\n",
    "Libraries used: please include the main libraries you used in your assignment here, e.g.,:\n",
    "* os (for reading the names of the file) \n",
    "* re (for regular expression, included in Anaconda Python 3.7.4) \n",
    "* langid (for detecting language of a text) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import langid\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading the data in\n",
    "\n",
    "\n",
    "* Using os.listdir to read only the filenames of all the 2,400 files in the directory specified by the user. \n",
    "\n",
    "* Use a relative path to access these directories\n",
    "\n",
    "* Passing this list of filenames while reading to make sure all the mentioned files are read "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_dir = os.listdir('C:/Users/Sai Ram/OneDrive/Semester-2/FIT5196/Assignment-1/Assignment-Data/')\n",
    "\n",
    "filenames = list_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading data from file1\n",
    "data1 = []\n",
    "for fname in filenames:\n",
    "    with open(fname, 'r', encoding='utf-8',errors='ignore') as fp:\n",
    "        data1.append(fp.readlines( ))\n",
    "\n",
    "    fp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning the read in data \n",
    "\n",
    "* Splitting the data on '},{' gives us an entire tweet which consists of the time it was created at, the tweet id and the tweet \n",
    "* The cleaning process does not end here\n",
    "\n",
    "### Then, for each tweet, cleaning the quotes (\" \") around several characters need to be done\n",
    "* For the ' \":\" ' colon part\n",
    "* For the ' \" created ' part\n",
    "* For the ' Z \" ' part\n",
    "* For the ' \" text ' part\n",
    "* For the id: part where the id: ends with a \" followed by a digit\n",
    "\n",
    "The above mentioned is carried out by using regex and replace functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst = list( ) # list of tweets \n",
    "x = str(data1).split('},{') # splitting on },{\n",
    "for i in range(len(x)): \n",
    "    z = x[i].split('\",\"')\n",
    "    lst.append(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(lst)):\n",
    "    for j in range(3):\n",
    "\n",
    "        lst[i][j] = lst[i][j].replace('\":\"', ':').replace('\"created', 'created').replace('Z\"', 'Z').replace \\\n",
    "            ('\"text', 'text')\n",
    "        lst[i][j] = lst[i][j].replace(','.join(re.findall(r'^id:\\d+\"', lst[i][j])).replace(',', ''),\n",
    "                                      ','.join(re.findall(r'^id:\\d+', lst[i][j])).replace(',', ''))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Data still needs to be cleaned!\n",
    "* At the begining of each files that is read, there are unwanted characters appended to them. \n",
    "\n",
    "Removal of which is essential and is carried out below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_lst = list( )\n",
    "for i in lst:\n",
    "    for j in range(len(i)):\n",
    "        if re.findall(r\"{\\\"data\\\":\", i[j]):\n",
    "            i[j] = i[j].replace(','.join(re.findall(r\"'\\[\\[\\\\'{\\\"data\\\":\\[\", i[j])).replace(',', ''), '')\n",
    "            i[j] = i[j].replace(','.join(re.findall(r\"\\\"\\[\\[\\'\\[{\", i[j])).replace(',',''),'')\n",
    "\n",
    "        i[j] = i[j].replace(\"\\n\\n\", \"\")\n",
    "        i[j] = i[j].replace(','.join(re.findall(r'\\[.', i[j])).replace(',', ''), '')\n",
    "\n",
    "        # i[j] = i[j].replace(','.join(re.findall(r'\\[{', i[j])).replace(',', ''), '[')\n",
    "\n",
    "        if re.match(r'id:\\d+', i[j]):\n",
    "            new_lst.append(i)\n",
    "\n",
    "        if re.findall(r'T\\d+:\\d+:\\d+.\\d+Z', i[j]):\n",
    "            i[j] = i[j].replace(','.join(re.findall(r'T\\d+:\\d+:\\d+.\\d+Z', i[j])).replace(',', ''), '')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Desired content extraction\n",
    "Instead of cleaning/removing everything that is not necessary, we can filter out what is necessary and store them. \n",
    "- since there are data such as \"errors\", \"resource id\", \"type\", \"section\", \"detail\" etc in the data along with the tweets, our focus will be to concentrate only on the created_at, id and text part\n",
    "\n",
    "- within few of the tweets there are id's which have more than 19 digits in them. Only the first 19 digits in them are extracted \n",
    "\n",
    "A list is created which would have all the created_at, id, text (one tweet) as a list of lists and sorted in ascending order of the string. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_lst = list()\n",
    "for i in new_lst:\n",
    "    mylist = i\n",
    "    rt = re.compile(\"text:.*\")\n",
    "    ri = re.compile(\"created_at:.*\")\n",
    "    rc = re.compile(\"id:.*\")\n",
    "    \n",
    "    newlist_t = list(filter(rt.match, mylist)) \n",
    "    newlist_i = list(filter(ri.match, mylist)) \n",
    "    newlist_c = list(filter(rc.match, mylist)) \n",
    "    \n",
    "    clean_lst.append(newlist_t + newlist_i + newlist_c)\n",
    "    \n",
    "sorted_list_tweets = list(map(lambda y: sorted(y), clean_lst))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in sorted_list_tweets:\n",
    "    i[1] = i[1].replace(i[1], ','.join(re.findall(r'^id:\\d+', i[1])).replace(',', '')) # Replacing looooooong ids with ids\n",
    "    i[0] = i[0].replace(i[0], ','.join(re.findall(r'^created_at:\\d+[-]\\d+[-]\\d+', i[0])).replace(',', ''))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## There are few tweets with just 2 elements in them\n",
    "- either id and text\n",
    "- or text and id\n",
    "- or date and id and so on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Getting rid of all the tweets with 2 elements in them\n",
    "for i in sorted_list_tweets:\n",
    "    if len(i) == 2:\n",
    "        sorted_list_tweets.pop(sorted_list_tweets.index(i))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding and decoding\n",
    "- converting all the unicodes to emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = re.compile(r\"\\\\+u\")\n",
    "\n",
    "for i in sorted_list_tweets:\n",
    "    \n",
    "    res = r.sub(r\"\\\\u\",i[2])\n",
    "\n",
    "    y = res.encode(\"utf-8\",'surrogatepass').decode('unicode-escape')\n",
    "    i[2] = y.encode('utf-16','surrogatepass').decode('utf-16')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language classification\n",
    "- retaining only the tweets that are english (takes about 15min to run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy = [i for i in sorted_list_tweets if langid.classify(i[2])[0] == 'en']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting the dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict1 = {}\n",
    "\n",
    "for j in dummy:\n",
    "    dict1[j[0]] = j[1]+j[2]\n",
    "\n",
    "dates = []\n",
    "content = []\n",
    "for key, value in dict1.items() :\n",
    "    dates.append(key)\n",
    "    content.append(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a key value pair\n",
    "- key: created_at\n",
    "- value: id and text\n",
    "\n",
    "for each date collecting all the ids and tweets that were tweeted and each tweet is separated by 3 starts (***): which will help in splitting the string later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tweets_list = []\n",
    "updated_date = []\n",
    "for i in sorted(dates):\n",
    "    string = ''\n",
    "    for j in dummy:\n",
    "        if j[0] == i:\n",
    "            updated_date.append(i)\n",
    "            string += j[1]+j[2]+'***'\n",
    "    tweets_list.append(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sorting the date in ascending order\n",
    "created_date = sorted(list(set(updated_date)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Creating a dictionary with key as the created_date and value as its entire day's tweets \n",
    "dictionary_dates_tweets = dict(zip(created_date, tweets_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Writing the entire data to an xml format\n",
    "- Here the dates, ids and texts are extracted using regex and the written to the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open('31009751.xml','w', encoding=\"utf-8\") as f:\n",
    "    f.write('<?xml version=\"1.0\" encoding=\"UTF-8\"?>\\n')\n",
    "    f.write('<data>\\n')\n",
    "    for i in created_date:\n",
    "        i = i.replace(i, re.findall(r'\\d{4}-\\d{2}-\\d{2}',i)[0])\n",
    "        f.write(f'<tweets date=\"{i}\">\\n')\n",
    "\n",
    "        for l in range(len(dictionary_dates_tweets['created_at:'+i].split('***'))):\n",
    "            if len(dictionary_dates_tweets['created_at:'+i].split('***')[l].split('text:')) == 2:\n",
    "                id_1 = re.findall(r'[0-9]{19}',dictionary_dates_tweets['created_at:'+i].split('***')[l].split('text:')[0])[0]\n",
    "                text_1 = dictionary_dates_tweets['created_at:'+i].split('***')[l].split('text:')[1]\n",
    "\n",
    "\n",
    "                f.write(f'<tweet id=\"{id_1}\">\"{text_1}\"</tweet>\\n')\n",
    "            else: pass\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
